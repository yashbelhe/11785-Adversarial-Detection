{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecondaryClassifier - Yash.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashbelhe/11785-Adversarial-Detection/blob/master/SecondaryClassifier_Yash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZsnU_j7Phnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsh6gXGPPi0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cc097e5-a6ed-4259-cd92-6a4e93f6eee8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj1Y9ZtzQNC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.fftpack import dct\n",
        "from PIL import Image\n",
        "\n",
        "def dct2(a):\n",
        "    return dct( dct( a, axis=0, norm='ortho' ), axis=1, norm='ortho' )\n",
        "\n",
        "def idct2(a):\n",
        "    return idct( idct( a, axis=0 , norm='ortho'), axis=1 , norm='ortho')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoPQb0RM9vFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "81b4be30-0297-42cb-f4e4-00607cab0a96"
      },
      "source": [
        "!pip install torch-dct"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch-dct in /usr/local/lib/python3.6/dist-packages (0.1.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from torch-dct) (1.3.0+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->torch-dct) (1.17.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owIAPaeyAs-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch_dct as dct\n",
        "from math import floor\n",
        "class RandomDCTDrop(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        dct_img = dct.dct_2d(img)\n",
        "        # dct_img_r = dct.dct_2d(img[0,:,:])\n",
        "        # dct_img_g = dct.dct_2d(img[1,:,:])\n",
        "        # dct_img_b = dct.dct_2d(img[2,:,:])\n",
        "\n",
        "        C, H, W = dct_img.shape\n",
        "        # H, W = dct_img_r.shape\n",
        "\n",
        "        mask = torch.zeros((H,W), dtype = bool)\n",
        "\n",
        "        # num_coeff = floor(self.prob*H*W)\n",
        "\n",
        "        # indices = np.random.randint(0,32,(int(num_coeff),2))\n",
        "        # indices = torch.randperm(H*W)[:num_coeff]\n",
        "        mask = torch.FloatTensor(H, W).uniform_() > self.prob\n",
        "\n",
        "        dct_img[:, mask] = 0\n",
        "        # dct_img_r[mask] = 0\n",
        "        # dct_img_g[mask] = 0\n",
        "        # dct_img_b[mask] = 0\n",
        "        # for i in range(len(indices)):\n",
        "        #     h = indices[i] % W\n",
        "        #     w = indices[i] // W\n",
        "        #     dct_img_r[h,w] = 0\n",
        "        #     dct_img_g[h,w] = 0\n",
        "        #     dct_img_b[h,w] = 0\n",
        "        \n",
        "        # img_r = dct.idct_2d(dct_img_r)\n",
        "        # img_g = dct.idct_2d(dct_img_g)\n",
        "        # img_b = dct.idct_2d(dct_img_b)\n",
        "\n",
        "        # dct_img = torch.stack([dct_img_r, dct_img_g, dct_img_b], dim=0)\n",
        "        img_mod = dct.idct_2d(dct_img)\n",
        "        return img_mod"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnLnB467cIzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a758348-b6de-493c-ff86-e01fea050acc"
      },
      "source": [
        " normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "# The argumet to DCTDrop is the probablility of keeping each coefficient\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # transforms.RandomCrop(32, 4),\n",
        "        transforms.ToTensor(),\n",
        "        RandomDCTDrop(0.6),\n",
        "        normalize,\n",
        "    ]), download=True),\n",
        "    batch_size=128, shuffle=True,\n",
        "    num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])),\n",
        "    batch_size=128, shuffle=False,\n",
        "    num_workers=4, pin_memory=True)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtVnWJDxDXkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "b06a3b22-8938-49ad-9efb-4a80cbc044d2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "for img, label in train_loader:\n",
        "    break;\n",
        "print(img.shape)\n",
        "# mean=torch.Tensor([0.485, 0.456, 0.406])\n",
        "# std=torch.Tensor([0.229, 0.224, 0.225])\n",
        "im = img[1]\n",
        "print(im.shape)\n",
        "# im = im*std + mean\n",
        "plt.imshow(im.numpy().transpose(1,2,0))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6572db1cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJwoEi50oEP8LmAV5/eGBuU8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqKBDbI-zPRo",
        "colab_type": "code",
        "outputId": "83807eb5-41ac-426a-f087-817db46b72c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/akamaster/pytorch_resnet_cifar10"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pytorch_resnet_cifar10' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg-0wHCxzVbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget https://github.com/akamaster/pytorch_resnet_cifar10/raw/master/pretrained_models/resnet20-12fca82f.th"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8P--YG50vEO",
        "colab_type": "code",
        "outputId": "1b62a400-2c80-4a83-e118-a7ee7e31d534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from pytorch_resnet_cifar10.resnet import *\n",
        "\n",
        "model = torch.nn.DataParallel(resnet20())\n",
        "model.cuda()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): LambdaLayer()\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): LambdaLayer()\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owsoxvWrdl4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), 0.1,\n",
        "                                momentum=0.9,\n",
        "                                weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "#                                                         milestones=[100, 150], last_epoch=0)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                        milestones=[100, 150])\n",
        "print_freq = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-oNtqRD_myb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xJK3EM_e3hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "        Run one train epoch\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SU-L4HDe_zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Run evaluation\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                          i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                          top1=top1))\n",
        "\n",
        "    print(' * Prec@1 {top1.avg:.3f}'\n",
        "          .format(top1=top1))\n",
        "\n",
        "    return top1.avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvVDWoE4fIAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    \"\"\"\n",
        "    Save the training model\n",
        "    \"\"\"\n",
        "    torch.save(state, filename)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqmMIeoQfLIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHONbQDafOow",
        "colab_type": "code",
        "outputId": "a6dd2a6e-c35a-42e9-ac18-f4f42fb4558e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_prec1 = 0\n",
        "for epoch in range(0, 200):\n",
        "\n",
        "        # train for one epoch\n",
        "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 > best_prec1\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "\n",
        "        if epoch > 0 and epoch % 10 == 0:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_prec1': best_prec1,\n",
        "            }, is_best, filename=os.path.join('/content/gdrive/My Drive/', 'checkpoint.th'))\n",
        "\n",
        "        save_checkpoint({\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_prec1': best_prec1,\n",
        "        }, is_best, filename=os.path.join('', 'model.th'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current lr 1.00000e-01\n",
            "Epoch: [0][0/391]\tTime 0.742 (0.742)\tData 0.691 (0.691)\tLoss 3.4488 (3.4488)\tPrec@1 10.938 (10.938)\n",
            "Epoch: [0][50/391]\tTime 0.173 (0.196)\tData 0.146 (0.161)\tLoss 2.2231 (2.5160)\tPrec@1 19.531 (15.119)\n",
            "Epoch: [0][100/391]\tTime 0.040 (0.189)\tData 0.000 (0.153)\tLoss 2.0133 (2.3113)\tPrec@1 20.312 (17.922)\n",
            "Epoch: [0][150/391]\tTime 0.517 (0.187)\tData 0.480 (0.152)\tLoss 1.9948 (2.2188)\tPrec@1 27.344 (19.511)\n",
            "Epoch: [0][200/391]\tTime 0.178 (0.183)\tData 0.143 (0.148)\tLoss 1.8339 (2.1463)\tPrec@1 28.125 (21.156)\n",
            "Epoch: [0][250/391]\tTime 0.433 (0.182)\tData 0.388 (0.148)\tLoss 1.7911 (2.0890)\tPrec@1 30.469 (22.887)\n",
            "Epoch: [0][300/391]\tTime 0.030 (0.182)\tData 0.000 (0.147)\tLoss 1.9109 (2.0449)\tPrec@1 27.344 (24.398)\n",
            "Epoch: [0][350/391]\tTime 0.033 (0.181)\tData 0.000 (0.146)\tLoss 1.7335 (2.0024)\tPrec@1 34.375 (25.757)\n",
            "Test: [0/79]\tTime 0.259 (0.259)\tLoss 1.6349 (1.6349)\tPrec@1 38.281 (38.281)\n",
            "Test: [50/79]\tTime 0.010 (0.023)\tLoss 1.6248 (1.7661)\tPrec@1 35.938 (34.145)\n",
            " * Prec@1 33.570\n",
            "current lr 1.00000e-01\n",
            "Epoch: [1][0/391]\tTime 0.813 (0.813)\tData 0.770 (0.770)\tLoss 1.8451 (1.8451)\tPrec@1 33.594 (33.594)\n",
            "Epoch: [1][50/391]\tTime 0.040 (0.189)\tData 0.000 (0.154)\tLoss 1.6857 (1.7036)\tPrec@1 38.281 (36.275)\n",
            "Epoch: [1][100/391]\tTime 0.029 (0.185)\tData 0.000 (0.148)\tLoss 1.6427 (1.6811)\tPrec@1 35.938 (37.252)\n",
            "Epoch: [1][150/391]\tTime 0.047 (0.184)\tData 0.000 (0.147)\tLoss 1.6499 (1.6508)\tPrec@1 41.406 (38.742)\n",
            "Epoch: [1][200/391]\tTime 0.051 (0.181)\tData 0.000 (0.144)\tLoss 1.4991 (1.6260)\tPrec@1 48.438 (39.867)\n",
            "Epoch: [1][250/391]\tTime 0.034 (0.181)\tData 0.005 (0.145)\tLoss 1.5497 (1.6109)\tPrec@1 42.188 (40.407)\n",
            "Epoch: [1][300/391]\tTime 0.035 (0.179)\tData 0.000 (0.142)\tLoss 1.4190 (1.5860)\tPrec@1 48.438 (41.489)\n",
            "Epoch: [1][350/391]\tTime 0.131 (0.178)\tData 0.102 (0.142)\tLoss 1.4126 (1.5693)\tPrec@1 46.094 (42.317)\n",
            "Test: [0/79]\tTime 0.232 (0.232)\tLoss 1.7675 (1.7675)\tPrec@1 37.500 (37.500)\n",
            "Test: [50/79]\tTime 0.022 (0.023)\tLoss 1.5297 (1.7397)\tPrec@1 48.438 (41.008)\n",
            " * Prec@1 40.770\n",
            "current lr 1.00000e-01\n",
            "Epoch: [2][0/391]\tTime 0.715 (0.715)\tData 0.676 (0.676)\tLoss 1.3682 (1.3682)\tPrec@1 52.344 (52.344)\n",
            "Epoch: [2][50/391]\tTime 0.081 (0.183)\tData 0.051 (0.148)\tLoss 1.4177 (1.4085)\tPrec@1 50.781 (49.142)\n",
            "Epoch: [2][100/391]\tTime 0.037 (0.180)\tData 0.000 (0.146)\tLoss 1.1552 (1.3882)\tPrec@1 57.812 (50.039)\n",
            "Epoch: [2][150/391]\tTime 0.036 (0.180)\tData 0.000 (0.146)\tLoss 1.4303 (1.3757)\tPrec@1 50.781 (50.507)\n",
            "Epoch: [2][200/391]\tTime 0.049 (0.180)\tData 0.000 (0.145)\tLoss 1.3102 (1.3664)\tPrec@1 55.469 (50.808)\n",
            "Epoch: [2][250/391]\tTime 0.041 (0.180)\tData 0.005 (0.145)\tLoss 1.2435 (1.3502)\tPrec@1 52.344 (51.584)\n",
            "Epoch: [2][300/391]\tTime 0.036 (0.179)\tData 0.000 (0.143)\tLoss 1.1313 (1.3348)\tPrec@1 64.844 (52.157)\n",
            "Epoch: [2][350/391]\tTime 0.034 (0.179)\tData 0.000 (0.144)\tLoss 1.1962 (1.3241)\tPrec@1 57.031 (52.620)\n",
            "Test: [0/79]\tTime 0.258 (0.258)\tLoss 1.3485 (1.3485)\tPrec@1 53.906 (53.906)\n",
            "Test: [50/79]\tTime 0.025 (0.024)\tLoss 1.3694 (1.4352)\tPrec@1 50.000 (51.945)\n",
            " * Prec@1 52.070\n",
            "current lr 1.00000e-01\n",
            "Epoch: [3][0/391]\tTime 0.819 (0.819)\tData 0.783 (0.783)\tLoss 1.1524 (1.1524)\tPrec@1 57.031 (57.031)\n",
            "Epoch: [3][50/391]\tTime 0.042 (0.184)\tData 0.000 (0.147)\tLoss 1.3355 (1.2148)\tPrec@1 54.688 (56.924)\n",
            "Epoch: [3][100/391]\tTime 0.444 (0.181)\tData 0.411 (0.145)\tLoss 1.0640 (1.1890)\tPrec@1 67.188 (57.681)\n",
            "Epoch: [3][150/391]\tTime 0.037 (0.179)\tData 0.000 (0.143)\tLoss 1.2522 (1.1716)\tPrec@1 53.125 (58.376)\n",
            "Epoch: [3][200/391]\tTime 0.042 (0.178)\tData 0.005 (0.142)\tLoss 1.2048 (1.1639)\tPrec@1 56.250 (58.605)\n",
            "Epoch: [3][250/391]\tTime 0.044 (0.178)\tData 0.005 (0.142)\tLoss 1.1506 (1.1590)\tPrec@1 61.719 (58.693)\n",
            "Epoch: [3][300/391]\tTime 0.038 (0.177)\tData 0.000 (0.141)\tLoss 1.2325 (1.1541)\tPrec@1 57.812 (58.975)\n",
            "Epoch: [3][350/391]\tTime 0.035 (0.177)\tData 0.000 (0.140)\tLoss 1.0751 (1.1449)\tPrec@1 62.500 (59.315)\n",
            "Test: [0/79]\tTime 0.247 (0.247)\tLoss 1.5940 (1.5940)\tPrec@1 48.438 (48.438)\n",
            "Test: [50/79]\tTime 0.011 (0.023)\tLoss 1.6979 (1.6817)\tPrec@1 53.125 (48.621)\n",
            " * Prec@1 49.080\n",
            "current lr 1.00000e-01\n",
            "Epoch: [4][0/391]\tTime 0.870 (0.870)\tData 0.829 (0.829)\tLoss 0.9843 (0.9843)\tPrec@1 62.500 (62.500)\n",
            "Epoch: [4][50/391]\tTime 0.037 (0.183)\tData 0.000 (0.146)\tLoss 1.0580 (1.0638)\tPrec@1 65.625 (62.975)\n",
            "Epoch: [4][100/391]\tTime 0.097 (0.180)\tData 0.065 (0.145)\tLoss 1.2730 (1.0661)\tPrec@1 63.281 (62.778)\n",
            "Epoch: [4][150/391]\tTime 0.044 (0.178)\tData 0.000 (0.142)\tLoss 1.1203 (1.0617)\tPrec@1 60.156 (62.795)\n",
            "Epoch: [4][200/391]\tTime 0.032 (0.177)\tData 0.000 (0.142)\tLoss 0.8837 (1.0525)\tPrec@1 71.094 (62.931)\n",
            "Epoch: [4][250/391]\tTime 0.032 (0.176)\tData 0.000 (0.141)\tLoss 1.0330 (1.0506)\tPrec@1 66.406 (63.166)\n",
            "Epoch: [4][300/391]\tTime 0.031 (0.177)\tData 0.005 (0.141)\tLoss 1.2163 (1.0423)\tPrec@1 58.594 (63.468)\n",
            "Epoch: [4][350/391]\tTime 0.038 (0.176)\tData 0.000 (0.141)\tLoss 1.0190 (1.0377)\tPrec@1 67.188 (63.626)\n",
            "Test: [0/79]\tTime 0.248 (0.248)\tLoss 1.0850 (1.0850)\tPrec@1 64.844 (64.844)\n",
            "Test: [50/79]\tTime 0.027 (0.023)\tLoss 1.1475 (1.1140)\tPrec@1 61.719 (63.143)\n",
            " * Prec@1 63.270\n",
            "current lr 1.00000e-01\n",
            "Epoch: [5][0/391]\tTime 0.778 (0.778)\tData 0.730 (0.730)\tLoss 0.9895 (0.9895)\tPrec@1 68.750 (68.750)\n",
            "Epoch: [5][50/391]\tTime 0.033 (0.183)\tData 0.000 (0.146)\tLoss 1.0110 (0.9668)\tPrec@1 67.969 (66.575)\n",
            "Epoch: [5][100/391]\tTime 0.557 (0.182)\tData 0.530 (0.146)\tLoss 0.9167 (0.9635)\tPrec@1 71.875 (66.615)\n",
            "Epoch: [5][150/391]\tTime 0.040 (0.179)\tData 0.000 (0.143)\tLoss 0.9363 (0.9663)\tPrec@1 65.625 (66.023)\n",
            "Epoch: [5][200/391]\tTime 0.460 (0.178)\tData 0.422 (0.141)\tLoss 0.9514 (0.9682)\tPrec@1 64.844 (66.014)\n",
            "Epoch: [5][250/391]\tTime 0.036 (0.176)\tData 0.000 (0.139)\tLoss 0.9315 (0.9690)\tPrec@1 64.844 (66.002)\n",
            "Epoch: [5][300/391]\tTime 0.610 (0.177)\tData 0.572 (0.141)\tLoss 0.8993 (0.9665)\tPrec@1 71.094 (66.116)\n",
            "Epoch: [5][350/391]\tTime 0.034 (0.177)\tData 0.000 (0.140)\tLoss 0.7753 (0.9644)\tPrec@1 71.094 (66.179)\n",
            "Test: [0/79]\tTime 0.254 (0.254)\tLoss 1.1147 (1.1147)\tPrec@1 67.969 (67.969)\n",
            "Test: [50/79]\tTime 0.016 (0.023)\tLoss 1.3741 (1.1704)\tPrec@1 55.469 (62.638)\n",
            " * Prec@1 62.420\n",
            "current lr 1.00000e-01\n",
            "Epoch: [6][0/391]\tTime 0.909 (0.909)\tData 0.869 (0.869)\tLoss 1.0346 (1.0346)\tPrec@1 62.500 (62.500)\n",
            "Epoch: [6][50/391]\tTime 0.037 (0.182)\tData 0.000 (0.145)\tLoss 0.9450 (0.9351)\tPrec@1 65.625 (67.402)\n",
            "Epoch: [6][100/391]\tTime 0.103 (0.179)\tData 0.072 (0.144)\tLoss 0.7826 (0.9175)\tPrec@1 71.875 (68.232)\n",
            "Epoch: [6][150/391]\tTime 0.042 (0.177)\tData 0.000 (0.141)\tLoss 0.9310 (0.9143)\tPrec@1 65.625 (68.103)\n",
            "Epoch: [6][200/391]\tTime 0.316 (0.177)\tData 0.282 (0.141)\tLoss 0.7700 (0.9145)\tPrec@1 77.344 (68.249)\n",
            "Epoch: [6][250/391]\tTime 0.083 (0.175)\tData 0.056 (0.139)\tLoss 1.0501 (0.9129)\tPrec@1 63.281 (68.308)\n",
            "Epoch: [6][300/391]\tTime 0.441 (0.176)\tData 0.403 (0.141)\tLoss 0.7132 (0.9119)\tPrec@1 75.000 (68.304)\n",
            "Epoch: [6][350/391]\tTime 0.036 (0.175)\tData 0.000 (0.140)\tLoss 0.8112 (0.9116)\tPrec@1 72.656 (68.343)\n",
            "Test: [0/79]\tTime 0.252 (0.252)\tLoss 1.3010 (1.3010)\tPrec@1 60.156 (60.156)\n",
            "Test: [50/79]\tTime 0.023 (0.023)\tLoss 1.6131 (1.4134)\tPrec@1 56.250 (59.743)\n",
            " * Prec@1 59.930\n",
            "current lr 1.00000e-01\n",
            "Epoch: [7][0/391]\tTime 0.744 (0.744)\tData 0.692 (0.692)\tLoss 0.7994 (0.7994)\tPrec@1 74.219 (74.219)\n",
            "Epoch: [7][50/391]\tTime 0.046 (0.189)\tData 0.000 (0.154)\tLoss 0.9621 (0.9043)\tPrec@1 71.094 (68.689)\n",
            "Epoch: [7][100/391]\tTime 0.033 (0.181)\tData 0.000 (0.146)\tLoss 0.9559 (0.9027)\tPrec@1 67.969 (68.572)\n",
            "Epoch: [7][150/391]\tTime 0.043 (0.181)\tData 0.000 (0.146)\tLoss 1.0292 (0.9019)\tPrec@1 66.406 (68.791)\n",
            "Epoch: [7][200/391]\tTime 0.047 (0.180)\tData 0.000 (0.144)\tLoss 0.9464 (0.9010)\tPrec@1 65.625 (68.777)\n",
            "Epoch: [7][250/391]\tTime 0.042 (0.180)\tData 0.000 (0.144)\tLoss 0.9530 (0.8929)\tPrec@1 65.625 (69.002)\n",
            "Epoch: [7][300/391]\tTime 0.053 (0.178)\tData 0.000 (0.142)\tLoss 0.6243 (0.8898)\tPrec@1 80.469 (69.186)\n",
            "Epoch: [7][350/391]\tTime 0.044 (0.178)\tData 0.000 (0.142)\tLoss 1.0066 (0.8832)\tPrec@1 64.062 (69.416)\n",
            "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.9591 (0.9591)\tPrec@1 65.625 (65.625)\n",
            "Test: [50/79]\tTime 0.017 (0.023)\tLoss 1.0294 (1.0456)\tPrec@1 68.750 (65.962)\n",
            " * Prec@1 66.020\n",
            "current lr 1.00000e-01\n",
            "Epoch: [8][0/391]\tTime 0.787 (0.787)\tData 0.747 (0.747)\tLoss 0.9795 (0.9795)\tPrec@1 68.750 (68.750)\n",
            "Epoch: [8][50/391]\tTime 0.262 (0.188)\tData 0.225 (0.154)\tLoss 0.9269 (0.8562)\tPrec@1 71.094 (70.573)\n",
            "Epoch: [8][100/391]\tTime 0.203 (0.180)\tData 0.173 (0.145)\tLoss 0.8795 (0.8682)\tPrec@1 75.781 (70.320)\n",
            "Epoch: [8][150/391]\tTime 0.256 (0.177)\tData 0.219 (0.143)\tLoss 0.7752 (0.8643)\tPrec@1 71.094 (70.240)\n",
            "Epoch: [8][200/391]\tTime 0.589 (0.176)\tData 0.556 (0.143)\tLoss 0.7474 (0.8681)\tPrec@1 70.312 (69.897)\n",
            "Epoch: [8][250/391]\tTime 0.040 (0.175)\tData 0.000 (0.140)\tLoss 0.8337 (0.8678)\tPrec@1 73.438 (70.039)\n",
            "Epoch: [8][300/391]\tTime 0.512 (0.175)\tData 0.476 (0.140)\tLoss 0.9553 (0.8682)\tPrec@1 68.750 (70.030)\n",
            "Epoch: [8][350/391]\tTime 0.041 (0.174)\tData 0.000 (0.139)\tLoss 0.8147 (0.8635)\tPrec@1 76.562 (70.212)\n",
            "Test: [0/79]\tTime 0.251 (0.251)\tLoss 1.0083 (1.0083)\tPrec@1 67.188 (67.188)\n",
            "Test: [50/79]\tTime 0.012 (0.023)\tLoss 1.1355 (0.9399)\tPrec@1 63.281 (69.730)\n",
            " * Prec@1 69.550\n",
            "current lr 1.00000e-01\n",
            "Epoch: [9][0/391]\tTime 0.945 (0.945)\tData 0.896 (0.896)\tLoss 0.7719 (0.7719)\tPrec@1 76.562 (76.562)\n",
            "Epoch: [9][50/391]\tTime 0.047 (0.191)\tData 0.000 (0.155)\tLoss 0.9553 (0.8317)\tPrec@1 67.969 (71.523)\n",
            "Epoch: [9][100/391]\tTime 0.628 (0.185)\tData 0.588 (0.148)\tLoss 0.8559 (0.8315)\tPrec@1 72.656 (71.465)\n",
            "Epoch: [9][150/391]\tTime 0.042 (0.180)\tData 0.000 (0.142)\tLoss 0.8279 (0.8370)\tPrec@1 71.875 (71.332)\n",
            "Epoch: [9][200/391]\tTime 0.620 (0.181)\tData 0.589 (0.143)\tLoss 0.7113 (0.8425)\tPrec@1 75.781 (71.032)\n",
            "Epoch: [9][250/391]\tTime 0.041 (0.178)\tData 0.000 (0.140)\tLoss 0.9416 (0.8443)\tPrec@1 64.062 (70.944)\n",
            "Epoch: [9][300/391]\tTime 0.594 (0.178)\tData 0.554 (0.141)\tLoss 0.9016 (0.8438)\tPrec@1 71.094 (70.987)\n",
            "Epoch: [9][350/391]\tTime 0.046 (0.177)\tData 0.000 (0.140)\tLoss 0.7442 (0.8413)\tPrec@1 73.438 (71.063)\n",
            "Test: [0/79]\tTime 0.262 (0.262)\tLoss 1.0385 (1.0385)\tPrec@1 70.312 (70.312)\n",
            "Test: [50/79]\tTime 0.030 (0.022)\tLoss 1.0895 (1.0186)\tPrec@1 70.312 (68.934)\n",
            " * Prec@1 69.210\n",
            "current lr 1.00000e-01\n",
            "Epoch: [10][0/391]\tTime 0.823 (0.823)\tData 0.782 (0.782)\tLoss 0.9196 (0.9196)\tPrec@1 66.406 (66.406)\n",
            "Epoch: [10][50/391]\tTime 0.042 (0.178)\tData 0.012 (0.144)\tLoss 0.6883 (0.8212)\tPrec@1 77.344 (71.124)\n",
            "Epoch: [10][100/391]\tTime 0.541 (0.180)\tData 0.508 (0.146)\tLoss 0.9307 (0.8204)\tPrec@1 67.969 (71.426)\n",
            "Epoch: [10][150/391]\tTime 0.221 (0.176)\tData 0.189 (0.141)\tLoss 0.8912 (0.8206)\tPrec@1 71.875 (71.420)\n",
            "Epoch: [10][200/391]\tTime 0.044 (0.176)\tData 0.000 (0.141)\tLoss 0.7862 (0.8231)\tPrec@1 73.438 (71.257)\n",
            "Epoch: [10][250/391]\tTime 0.609 (0.177)\tData 0.577 (0.142)\tLoss 0.8566 (0.8200)\tPrec@1 67.188 (71.501)\n",
            "Epoch: [10][300/391]\tTime 0.031 (0.175)\tData 0.000 (0.140)\tLoss 0.7692 (0.8229)\tPrec@1 75.781 (71.449)\n",
            "Epoch: [10][350/391]\tTime 0.032 (0.174)\tData 0.000 (0.139)\tLoss 0.8420 (0.8250)\tPrec@1 74.219 (71.434)\n",
            "Test: [0/79]\tTime 0.220 (0.220)\tLoss 1.6022 (1.6022)\tPrec@1 55.469 (55.469)\n",
            "Test: [50/79]\tTime 0.015 (0.023)\tLoss 1.8132 (1.5730)\tPrec@1 55.469 (56.725)\n",
            " * Prec@1 56.800\n",
            "current lr 1.00000e-01\n",
            "Epoch: [11][0/391]\tTime 0.800 (0.800)\tData 0.763 (0.763)\tLoss 0.7053 (0.7053)\tPrec@1 76.562 (76.562)\n",
            "Epoch: [11][50/391]\tTime 0.032 (0.182)\tData 0.000 (0.145)\tLoss 0.9038 (0.8025)\tPrec@1 70.312 (71.998)\n",
            "Epoch: [11][100/391]\tTime 0.525 (0.180)\tData 0.488 (0.145)\tLoss 0.8242 (0.8156)\tPrec@1 70.312 (71.705)\n",
            "Epoch: [11][150/391]\tTime 0.033 (0.177)\tData 0.000 (0.142)\tLoss 0.7722 (0.8111)\tPrec@1 75.781 (72.046)\n",
            "Epoch: [11][200/391]\tTime 0.601 (0.178)\tData 0.567 (0.142)\tLoss 0.8041 (0.8145)\tPrec@1 75.781 (71.902)\n",
            "Epoch: [11][250/391]\tTime 0.035 (0.176)\tData 0.000 (0.141)\tLoss 0.8867 (0.8132)\tPrec@1 65.625 (71.909)\n",
            "Epoch: [11][300/391]\tTime 0.636 (0.176)\tData 0.609 (0.140)\tLoss 0.7336 (0.8152)\tPrec@1 75.781 (71.828)\n",
            "Epoch: [11][350/391]\tTime 0.037 (0.175)\tData 0.000 (0.139)\tLoss 0.9362 (0.8129)\tPrec@1 66.406 (71.911)\n",
            "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.8064 (0.8064)\tPrec@1 71.875 (71.875)\n",
            "Test: [50/79]\tTime 0.021 (0.022)\tLoss 1.0440 (1.0085)\tPrec@1 68.750 (69.424)\n",
            " * Prec@1 69.500\n",
            "current lr 1.00000e-01\n",
            "Epoch: [12][0/391]\tTime 0.895 (0.895)\tData 0.849 (0.849)\tLoss 0.7407 (0.7407)\tPrec@1 74.219 (74.219)\n",
            "Epoch: [12][50/391]\tTime 0.039 (0.185)\tData 0.000 (0.147)\tLoss 0.9574 (0.8062)\tPrec@1 69.531 (72.135)\n",
            "Epoch: [12][100/391]\tTime 0.603 (0.180)\tData 0.560 (0.143)\tLoss 0.7520 (0.7800)\tPrec@1 75.781 (73.205)\n",
            "Epoch: [12][150/391]\tTime 0.046 (0.175)\tData 0.000 (0.138)\tLoss 0.8752 (0.7963)\tPrec@1 66.406 (72.444)\n",
            "Epoch: [12][200/391]\tTime 0.432 (0.174)\tData 0.401 (0.137)\tLoss 0.8463 (0.7918)\tPrec@1 68.750 (72.648)\n",
            "Epoch: [12][250/391]\tTime 0.034 (0.173)\tData 0.000 (0.136)\tLoss 0.8502 (0.7902)\tPrec@1 74.219 (72.666)\n",
            "Epoch: [12][300/391]\tTime 0.626 (0.174)\tData 0.592 (0.137)\tLoss 0.8755 (0.7925)\tPrec@1 69.531 (72.597)\n",
            "Epoch: [12][350/391]\tTime 0.041 (0.172)\tData 0.000 (0.135)\tLoss 0.9219 (0.7915)\tPrec@1 68.750 (72.587)\n",
            "Test: [0/79]\tTime 0.251 (0.251)\tLoss 1.1016 (1.1016)\tPrec@1 66.406 (66.406)\n",
            "Test: [50/79]\tTime 0.019 (0.022)\tLoss 1.2683 (1.2102)\tPrec@1 64.844 (64.691)\n",
            " * Prec@1 64.880\n",
            "current lr 1.00000e-01\n",
            "Epoch: [13][0/391]\tTime 0.817 (0.817)\tData 0.775 (0.775)\tLoss 0.8116 (0.8116)\tPrec@1 74.219 (74.219)\n",
            "Epoch: [13][50/391]\tTime 0.328 (0.183)\tData 0.297 (0.149)\tLoss 0.8386 (0.7817)\tPrec@1 72.656 (73.667)\n",
            "Epoch: [13][100/391]\tTime 0.034 (0.174)\tData 0.000 (0.139)\tLoss 0.8076 (0.7742)\tPrec@1 72.656 (73.708)\n",
            "Epoch: [13][150/391]\tTime 0.041 (0.173)\tData 0.000 (0.139)\tLoss 0.7860 (0.7882)\tPrec@1 71.094 (73.008)\n",
            "Epoch: [13][200/391]\tTime 0.419 (0.173)\tData 0.387 (0.139)\tLoss 0.7319 (0.7831)\tPrec@1 73.438 (73.173)\n",
            "Epoch: [13][250/391]\tTime 0.044 (0.173)\tData 0.000 (0.138)\tLoss 0.7833 (0.7890)\tPrec@1 71.875 (72.905)\n",
            "Epoch: [13][300/391]\tTime 0.045 (0.172)\tData 0.000 (0.137)\tLoss 0.8586 (0.7844)\tPrec@1 70.312 (72.983)\n",
            "Epoch: [13][350/391]\tTime 0.039 (0.173)\tData 0.000 (0.138)\tLoss 0.7426 (0.7861)\tPrec@1 75.781 (72.825)\n",
            "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.9314 (0.9314)\tPrec@1 74.219 (74.219)\n",
            "Test: [50/79]\tTime 0.019 (0.022)\tLoss 0.8945 (0.9652)\tPrec@1 70.312 (70.267)\n",
            " * Prec@1 69.890\n",
            "current lr 1.00000e-01\n",
            "Epoch: [14][0/391]\tTime 0.787 (0.787)\tData 0.741 (0.741)\tLoss 0.9058 (0.9058)\tPrec@1 64.844 (64.844)\n",
            "Epoch: [14][50/391]\tTime 0.263 (0.182)\tData 0.235 (0.149)\tLoss 0.6803 (0.7650)\tPrec@1 78.125 (73.453)\n",
            "Epoch: [14][100/391]\tTime 0.483 (0.177)\tData 0.451 (0.143)\tLoss 0.6033 (0.7612)\tPrec@1 78.125 (73.700)\n",
            "Epoch: [14][150/391]\tTime 0.405 (0.174)\tData 0.367 (0.140)\tLoss 0.8696 (0.7729)\tPrec@1 72.656 (73.246)\n",
            "Epoch: [14][200/391]\tTime 0.537 (0.173)\tData 0.502 (0.139)\tLoss 0.8293 (0.7810)\tPrec@1 72.656 (72.854)\n",
            "Epoch: [14][250/391]\tTime 0.035 (0.172)\tData 0.000 (0.137)\tLoss 0.8541 (0.7815)\tPrec@1 67.188 (72.747)\n",
            "Epoch: [14][300/391]\tTime 0.439 (0.172)\tData 0.411 (0.137)\tLoss 0.9004 (0.7803)\tPrec@1 69.531 (72.729)\n",
            "Epoch: [14][350/391]\tTime 0.037 (0.170)\tData 0.000 (0.135)\tLoss 0.7363 (0.7794)\tPrec@1 76.562 (72.745)\n",
            "Test: [0/79]\tTime 0.241 (0.241)\tLoss 1.0626 (1.0626)\tPrec@1 67.969 (67.969)\n",
            "Test: [50/79]\tTime 0.025 (0.023)\tLoss 1.0627 (0.9390)\tPrec@1 67.188 (70.037)\n",
            " * Prec@1 69.530\n",
            "current lr 1.00000e-01\n",
            "Epoch: [15][0/391]\tTime 0.722 (0.722)\tData 0.675 (0.675)\tLoss 0.6672 (0.6672)\tPrec@1 76.562 (76.562)\n",
            "Epoch: [15][50/391]\tTime 0.450 (0.185)\tData 0.417 (0.150)\tLoss 0.6724 (0.7577)\tPrec@1 78.125 (74.188)\n",
            "Epoch: [15][100/391]\tTime 0.098 (0.176)\tData 0.059 (0.143)\tLoss 0.6847 (0.7543)\tPrec@1 71.875 (74.366)\n",
            "Epoch: [15][150/391]\tTime 0.072 (0.174)\tData 0.031 (0.141)\tLoss 0.7719 (0.7613)\tPrec@1 72.656 (73.888)\n",
            "Epoch: [15][200/391]\tTime 0.034 (0.173)\tData 0.000 (0.140)\tLoss 0.8330 (0.7686)\tPrec@1 73.438 (73.511)\n",
            "Epoch: [15][250/391]\tTime 0.168 (0.173)\tData 0.142 (0.140)\tLoss 0.9024 (0.7674)\tPrec@1 71.094 (73.543)\n",
            "Epoch: [15][300/391]\tTime 0.153 (0.172)\tData 0.120 (0.139)\tLoss 0.8247 (0.7681)\tPrec@1 72.656 (73.445)\n",
            "Epoch: [15][350/391]\tTime 0.316 (0.172)\tData 0.284 (0.138)\tLoss 0.7944 (0.7665)\tPrec@1 71.094 (73.533)\n",
            "Test: [0/79]\tTime 0.238 (0.238)\tLoss 1.1696 (1.1696)\tPrec@1 64.062 (64.062)\n",
            "Test: [50/79]\tTime 0.022 (0.022)\tLoss 1.5103 (1.2207)\tPrec@1 57.812 (63.909)\n",
            " * Prec@1 64.280\n",
            "current lr 1.00000e-01\n",
            "Epoch: [16][0/391]\tTime 0.770 (0.770)\tData 0.724 (0.724)\tLoss 0.7295 (0.7295)\tPrec@1 72.656 (72.656)\n",
            "Epoch: [16][50/391]\tTime 0.031 (0.178)\tData 0.000 (0.143)\tLoss 0.7731 (0.7606)\tPrec@1 72.656 (73.468)\n",
            "Epoch: [16][100/391]\tTime 0.537 (0.181)\tData 0.503 (0.145)\tLoss 0.7086 (0.7555)\tPrec@1 71.875 (73.685)\n",
            "Epoch: [16][150/391]\tTime 0.043 (0.177)\tData 0.000 (0.140)\tLoss 0.5876 (0.7602)\tPrec@1 81.250 (73.531)\n",
            "Epoch: [16][200/391]\tTime 0.578 (0.178)\tData 0.550 (0.141)\tLoss 0.7120 (0.7621)\tPrec@1 75.000 (73.519)\n",
            "Epoch: [16][250/391]\tTime 0.036 (0.176)\tData 0.000 (0.139)\tLoss 0.7892 (0.7610)\tPrec@1 71.094 (73.497)\n",
            "Epoch: [16][300/391]\tTime 0.460 (0.175)\tData 0.427 (0.139)\tLoss 0.7148 (0.7574)\tPrec@1 77.344 (73.645)\n",
            "Epoch: [16][350/391]\tTime 0.042 (0.175)\tData 0.000 (0.138)\tLoss 0.6766 (0.7593)\tPrec@1 78.125 (73.613)\n",
            "Test: [0/79]\tTime 0.216 (0.216)\tLoss 0.9884 (0.9884)\tPrec@1 64.844 (64.844)\n",
            "Test: [50/79]\tTime 0.018 (0.022)\tLoss 1.0273 (1.0677)\tPrec@1 66.406 (67.172)\n",
            " * Prec@1 66.790\n",
            "current lr 1.00000e-01\n",
            "Epoch: [17][0/391]\tTime 0.786 (0.786)\tData 0.739 (0.739)\tLoss 0.7426 (0.7426)\tPrec@1 75.000 (75.000)\n",
            "Epoch: [17][50/391]\tTime 0.034 (0.181)\tData 0.000 (0.149)\tLoss 0.7861 (0.7459)\tPrec@1 75.781 (74.203)\n",
            "Epoch: [17][100/391]\tTime 0.219 (0.178)\tData 0.187 (0.145)\tLoss 0.6821 (0.7496)\tPrec@1 78.906 (74.257)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYxFDUIhKCgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_prec1': best_prec1,\n",
        "            }, is_best, filename=os.path.join('/content/gdrive/My Drive/', 'checkpoint_final_dct_drop.th'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnqF-eqpPsiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}